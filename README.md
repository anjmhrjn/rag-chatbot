# Personal Portfolio RAG Chatbot

A production-style **Retrieval-Augmented Generation (RAG)** chatbot that answers questions strictly about my background, projects, and certifications. Built with **FastAPI**, **FAISS**, and LLMs, the system prioritizes grounded responses, low cost, and safety.

This project is designed to be used as an interactive assistant on my portfolio website.

---

## âœ¨ Key Features

* **Grounded answers only** â€“ responses are generated strictly from retrieved context
* **Metadata-aware retrieval** â€“ project-specific filtering (LMS, RAG, Policy Navigator, etc.)
* **Cost-aware design** â€“ request gating, caching, and token caps
* **FastAPI backend** â€“ clean API boundaries and schemas
* **FAISS vector store** â€“ efficient semantic search
* **Production-minded guards** â€“ refusal on out-of-scope queries

---

## ğŸ§  Architecture Overview

```
User Query
   â†“
Topic & Safety Gates
   â†“
Query Embedding
   â†“
FAISS Vector Search
   â†“
Metadata Filtering
   â†“
Context Injection
   â†“
LLM Response (Token-Capped)
```

If no relevant context is found, the system **does not call the LLM**.

---

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app entry point
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ chat.py           # /chat endpoint
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ chat.py           # Pydantic request/response models
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ chunking.py       # Chunk documents
â”‚   â”‚   â”œâ”€â”€ embeddings.py     # Embedding logic
â”‚   â”‚   â”œâ”€â”€ retriever.py      # Retrieval + metadata filtering
â”‚   â”‚   â””â”€â”€ vectorstore.py    # FAISS wrapper
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ loader.py         # Document loading & chunking
â”‚   â”‚   â””â”€â”€ openai.py         # openai api request
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ build_index.py            # Offline index builder
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ resume.md             # markdown of example document
â”œâ”€â”€ index/
â”‚   â”œâ”€â”€ faiss.index
â”‚   â”œâ”€â”€ metadata.pkl
â”‚   â””â”€â”€ texts.pkl
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Build the FAISS index

```bash
python build_index.py
```

This step embeds documents and creates:

* `faiss.index`
* `metadata.pkl`
* `texts.pkl`

---

### 4. Run the API server

```bash
uvicorn app.main:app --reload
```

Open Swagger UI:

```
http://127.0.0.1:8000/docs
```

---

## ğŸ§ª Example API Request

```bash
curl -X POST http://127.0.0.1:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "Explain your Policy Navigator project"}'
```

### Example Response

```json
{
  "answer": "Policy Navigator is a project that ..."
}
```

---

## ğŸ›¡ï¸ Safety & Cost Controls

* âŒ No LLM calls for out-of-scope questions
* âŒ No LLM calls when retrieval returns no context
* âœ… Output token cap (`max_tokens=200`)
* âœ… Context size limits (chunks + characters)
* âœ… In-memory caching for repeated queries
* âœ… Keyword-based topic gating

> The cheapest LLM request is the one you never make.

---

## ğŸ”® Planned Improvements

* Cross-encoder re-ranking
* Source citations per answer
* Redis-based caching
* Streaming responses

---

## ğŸ“„ License

MIT License

---
